[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephanie J. Lao",
    "section": "",
    "text": "I attended graduate school at the University of California, Los Angeles where I obtained my master’s degree in applied statistics and data science. Previously, I obtained my bachelor’s degree in mathematics and biochemistry with a minor in art from St. Edward’s University in Austin, Texas. There, I was awarded a Welch Foundation research scholarship and a presidential merit scholarship.\nI have worked in industry as a data analyst and have conducted research in computational epidemiology as well as environmental data science.\nIn my free time, I enjoy working on my garden, trying new baking recipes, checking out different Pilates studios, swimming, and of course spending time with my family and friends."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Stephanie J. Lao",
    "section": "",
    "text": "I attended graduate school at the University of California, Los Angeles where I obtained my master’s degree in applied statistics and data science. Previously, I obtained my bachelor’s degree in mathematics and biochemistry with a minor in art from St. Edward’s University in Austin, Texas. There, I was awarded a Welch Foundation research scholarship and a presidential merit scholarship.\nI have worked in industry as a data analyst and have conducted research in computational epidemiology as well as environmental data science.\nIn my free time, I enjoy working on my garden, trying new baking recipes, checking out different Pilates studios, swimming, and of course spending time with my family and friends."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Selected Papers",
    "section": "",
    "text": "View it on eScholarship\nView it on ProQuest\nMaster’s Thesis at University of California, Los Angeles\nAdvisor: Xiaowu Dai\nMathematical models used in epidemiology, such as compartmental models, have the power to reveal the risk of infectious disease outbreaks and can impact the course of action for public health interventions. Despite these models being inexact at capturing the complexities of real-world disease transmission, they are capable of providing meaningful insight into physical systems and can ultimately support decision-making. These models use input parameters that represent characteristics of the physical system of interest. However, the true values of these parameters are often unknown because they cannot be directly measured. Hence, statistical calibration, or the process of identifying the optimal values of these parameters to best fit the observed data, is utilized to improve predictions and reliability of the model.\nAlthough declared an eradicated disease in the United States in 2000, there has been recent attention on several measles outbreaks throughout the country, most notably an outbreak beginning in Gaines County, Texas at the start of 2025. Here, we employ a dynamic compartmental model, known as an SVEIR model, to study the transmission of measles. We perform four different statistical calibration methods on the SVEIR model using daily incidence data from the West Texas outbreak collected by the Texas Department of State Health Services. We report estimated parameter values and basic reproduction numbers for the model calibrated using different methods. We give a comparison of predictive root mean square errors of all calibration methods and find that the optimal prediction calibration method returned the lowest predictive error. This study highlights the differences in calibration methods, hopes to provide an improved understanding of the current state of the ongoing measles outbreak in West Texas, and underscores avenues for future analysis."
  },
  {
    "objectID": "projects.html#statistical-calibration-of-a-compartmental-epidemic-model-with-applications-to-the-west-texas-measles-outbreak",
    "href": "projects.html#statistical-calibration-of-a-compartmental-epidemic-model-with-applications-to-the-west-texas-measles-outbreak",
    "title": "Selected Papers",
    "section": "",
    "text": "View it on eScholarship\nView it on ProQuest\nMaster’s Thesis at University of California, Los Angeles\nAdvisor: Xiaowu Dai\nMathematical models used in epidemiology, such as compartmental models, have the power to reveal the risk of infectious disease outbreaks and can impact the course of action for public health interventions. Despite these models being inexact at capturing the complexities of real-world disease transmission, they are capable of providing meaningful insight into physical systems and can ultimately support decision-making. These models use input parameters that represent characteristics of the physical system of interest. However, the true values of these parameters are often unknown because they cannot be directly measured. Hence, statistical calibration, or the process of identifying the optimal values of these parameters to best fit the observed data, is utilized to improve predictions and reliability of the model.\nAlthough declared an eradicated disease in the United States in 2000, there has been recent attention on several measles outbreaks throughout the country, most notably an outbreak beginning in Gaines County, Texas at the start of 2025. Here, we employ a dynamic compartmental model, known as an SVEIR model, to study the transmission of measles. We perform four different statistical calibration methods on the SVEIR model using daily incidence data from the West Texas outbreak collected by the Texas Department of State Health Services. We report estimated parameter values and basic reproduction numbers for the model calibrated using different methods. We give a comparison of predictive root mean square errors of all calibration methods and find that the optimal prediction calibration method returned the lowest predictive error. This study highlights the differences in calibration methods, hopes to provide an improved understanding of the current state of the ongoing measles outbreak in West Texas, and underscores avenues for future analysis."
  },
  {
    "objectID": "projects.html#predicting-deep-sea-coral-growth-in-davidson-seamount-using-gradient-boosted-machines",
    "href": "projects.html#predicting-deep-sea-coral-growth-in-davidson-seamount-using-gradient-boosted-machines",
    "title": "Selected Projects",
    "section": "Predicting Deep Sea Coral Growth in Davidson Seamount Using Gradient Boosted Machines",
    "text": "Predicting Deep Sea Coral Growth in Davidson Seamount Using Gradient Boosted Machines\nUndergraduate Capstone at St. Edward’s University\nAdvisor: Paul Savala\nAlthough the largest ecosystem on Earth, the deep ocean is also the least explored and understood. Hence, we are still in need of properly integrated biodiversity studies to unlock the patterns controlling species diversity in cold-water coral habitats. This study is specific to Davidson Seamount, a volcano located about 80 kilometers off the central California coast in the Monterey Bay National Marine Sanctuary. Predictive modeling has only recently been developed and applied in the marine environment due to the remoteness and limited accessibility of this environment. The objectives of this study were to develop and validate interpretable models to predict the occurrence of deep sea species of coral and sponges in Davidson Seamount.\nFor this study we utilized a subset tailored to Davidson Seamount of coral records from the Deep-Sea Coral Research and Technology Program of the National Oceanic and Atmospheric Administration (NOAA). The dataset contains information about deep sea corals and sponges collected by NOAA and their partners and is specifically tailored to the occurrences of azooxanthellates. Additionally, we utilized hydrographic records obtained from California Cooperative Oceanic Fisheries Investigations (CalCOFI). CalCOFI hydrographic data consists of the physical seawater properties measured over a 69 year time-series. The dataset contains seawater measurements from bottle sample depths. Our precise implementation will take the form of a glassbox explainable boosting machine, which are a special form of gradient boosted decision trees. The present research is the first to investigate coral growth in Davidson Seamount using interpretable models. Our results show that the most important factors influencing coral growth given the data include depth, temperature, salinity, dynamic height, and potential density. Our model reveals the most favorable conditions involving these factors for the occurrence of different coral in Davidson Seamount."
  },
  {
    "objectID": "projects.html#interpretable-predictive-modeling-of-factors-influencing-deep-sea-coral-growth-in-davidson-seamount",
    "href": "projects.html#interpretable-predictive-modeling-of-factors-influencing-deep-sea-coral-growth-in-davidson-seamount",
    "title": "Selected Papers",
    "section": "Interpretable Predictive Modeling of Factors Influencing Deep Sea Coral Growth in Davidson Seamount",
    "text": "Interpretable Predictive Modeling of Factors Influencing Deep Sea Coral Growth in Davidson Seamount\nUndergraduate Capstone at St. Edward’s University\nAdvisor: Paul Savala\nAlthough the largest ecosystem on Earth, the deep ocean is also the least explored and understood. Hence, we are still in need of properly integrated biodiversity studies to unlock the patterns controlling species diversity in cold-water coral habitats. This study is specific to Davidson Seamount, a volcano located about 80 kilometers off the central California coast in the Monterey Bay National Marine Sanctuary. Predictive modeling has only recently been developed and applied in the marine environment due to the remoteness and limited accessibility of this environment. The objectives of this study are to develop and validate interpretable models to predict the occurrence of deep sea species of coral and sponges in Davidson Seamount.\nFor this study we utilize a subset tailored to Davidson Seamount of coral records from the Deep-Sea Coral Research and Technology Program of the National Oceanic and Atmospheric Administration (NOAA). The dataset contains information about deep sea corals and sponges collected by NOAA and their partners and is specifically tailored to the occurrences of azooxanthellates. Additionally, we utilized hydrographic records obtained from California Cooperative Oceanic Fisheries Investigations (CalCOFI). CalCOFI hydrographic data consists of the physical seawater properties measured over a 69 year time-series. The dataset contains seawater measurements from bottle sample depths. Our precise implementation takes the form of a glass-box explainable boosting machine, which are a special form of gradient boosted decision trees. Our results show that the most important factors influencing coral growth given the data include depth, temperature, salinity, dynamic height, and potential density. Our model reveals the most favorable conditions involving these factors for the occurrence of different coral in Davidson Seamount."
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "R\nPython\nC\nJava\nSQL"
  },
  {
    "objectID": "skills.html#programming-languages",
    "href": "skills.html#programming-languages",
    "title": "Skills",
    "section": "",
    "text": "R\nPython\nC\nJava\nSQL"
  },
  {
    "objectID": "skills.html#softwaredatabase-managementother",
    "href": "skills.html#softwaredatabase-managementother",
    "title": "Skills",
    "section": "Software/Database Management/Other:",
    "text": "Software/Database Management/Other:\n\nTableau\nSnowflake\nMySQL\nGit\nExcel\nTeX\nAlteryx\nUiPath"
  },
  {
    "objectID": "skills.html#machine-learning-modeling",
    "href": "skills.html#machine-learning-modeling",
    "title": "Skills",
    "section": "Machine Learning Modeling:",
    "text": "Machine Learning Modeling:\n\nregression analysis (simple/multivariate linear regression, polynomial regression, logistic regression, ridge/lasso regression, kernel regression, generalized additive models)\ntree-based models (decision trees, random forests, gradient boosting)\nclustering (k-means clustering)\nsupport vector machines\nk-nearest neighbors\nartificial neural networks (FNNs, CNNs, RNNs, GANs)"
  },
  {
    "objectID": "skills.html#statisticsdata-science",
    "href": "skills.html#statisticsdata-science",
    "title": "Skills",
    "section": "Statistics/Data Science",
    "text": "Statistics/Data Science\n\ndata pre-processing\ndata visualization\ndata management\nprincipal component analysis\nprobability distributions\nmultivariate analysis\nexperimental design\nsurvey design\nhypothesis testing\nparametric and non-parametric tests\nresampling\ntime-series analysis\nstochastic modeling\nnatural language processing (sentiment analysis, topic modeling)\nkernel density estimation"
  },
  {
    "objectID": "skills.html#mathematics",
    "href": "skills.html#mathematics",
    "title": "Skills",
    "section": "Mathematics",
    "text": "Mathematics\n\ncalculus\ndiscrete mathematics\nlinear algebra\nabstract algebra\ndifferential equations\nprobability theory\nreal analysis\ncomplex analysis"
  },
  {
    "objectID": "skills.html#machine-learning-models",
    "href": "skills.html#machine-learning-models",
    "title": "Skills",
    "section": "Machine Learning Models",
    "text": "Machine Learning Models\n\nregression analysis (simple/multivariate linear regression, polynomial regression, logistic regression, ridge/lasso regression, kernel regression, generalized additive models)\ntree-based models (decision trees, random forests, gradient boosting)\nclustering (k-means clustering)\nsupport vector machines\nk-nearest neighbors\nartificial neural networks (FNNs, CNNs, RNNs, GANs)\ntransformers"
  },
  {
    "objectID": "posts/exponential_families.html",
    "href": "posts/exponential_families.html",
    "title": "Exponential Families",
    "section": "",
    "text": "Introduction\nThe exponential family is a mathematically convenient set of distributions, including the normal, Poisson, exponential, and gamma distributions as special cases. Outside of its wide use in classical statistical theory, its importance also reaches machine learning, such as in generalized linear models (GLMs).\nExpressing different probability density functions (pdf) or probability mass functions (pmf) in the form of an exponential family gives a convenient way to find the mean and variance of a random variable. We will give a brief introduction to the exponential family by providing a theorem that gives the mean and variance of a random variable belonging to an exponential family in terms of first and second derivatives, and include a two-part proof of this theorem. We will also give working examples to illustrate the theorem using the Poisson distribution and the binomial distribution with a fixed number of trials.\nDefinition: An exponential family is a family of distributions whose pdf or pmf can be expressed in the form\n\\[\\begin{equation}\n    f(x|\\theta) = h(x) c(\\theta) \\exp{ \\left ( \\sum_{i=1}^k w_i (\\theta) t_i (x) \\right )}\n\\end{equation}\\]\nwhere \\(h(x)\\) and \\(t_i(x)\\) do not depend on \\(\\theta\\) and \\(c(\\theta)\\) does not depend on \\(x\\).\nThe following theorem gives the mean and variance of a random variable with a pdf or pmf that can be expressed in the form of an exponential family.\nTheorem 1: Suppose a random variable \\(X\\) has a pdf or pmf that can be expressed in the form of an exponential family. Then,\n\\[\\begin{equation}\n\\textrm{(a)} \\  E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) = - \\frac{\\partial}{\\partial \\theta_j} \\ln c(\\theta)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\textrm{(b)} \\  \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) = - \\frac{\\partial^2}{\\partial \\theta_j^2} \\ln c(\\theta) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 w_i (\\theta)}{\\partial \\theta_j^2} t_i(x) \\right )\n\\end{equation}\\]\n\n\nProof of Theorem 1(a)\nTo prove part (a) of Theorem 1, we start by using the knowledge that the area under the pdf curve is 1 and the definition of exponential families to see that\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int_x f(x|\\theta) dx &= 1 \\\\\n    \\int_x h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx &= 1 \\\\\n\\end{aligned}\n\\end{equation*}\\]\nWe differentiate both sides of the above equation with respect to \\(\\theta_j\\) to get\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x h(x) \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\int_x h(x) c(\\theta) \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx = 0\n\\end{aligned}\n\\end{equation*}\\]\nWe then multiply the first integral by \\(\\frac{c(\\theta)}{c(\\theta)}\\) to get\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x h(x) \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) \\frac{c(\\theta)}{c(\\theta)} dx \\\\\n    + &\\int_x h(x) c(\\theta) \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx = 0\n\\end{aligned}\n\\end{equation*}\\]\nUsing the knowledge that \\(\\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} = \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\frac{1}{c(\\theta)}\\), see that\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x h(x) c(\\theta) \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    + &\\int_x h(x) c(\\theta) \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx = 0\n\\end{aligned}\n\\end{equation*}\\]\nAfter rearranging, we get\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx &= \\\\\n    - \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n\\end{aligned}\n\\end{equation*}\\]\nFinally, see that this is equivalent to\n\\[\\begin{equation*}\n\\begin{aligned}\n    E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) = - \\frac{\\partial}{\\partial \\theta_j} \\ln c(\\theta)\n\\end{aligned}\n\\end{equation*}\\]\n\n\n\nProof of Theorem 1(b)\nTo prove part (b) of Theorem 1, we differentiate a second time. From part (a) we found that\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    &= 0\\\\\n\\end{aligned}\n\\end{equation*}\\]\nWe differentiate both sides of the above equation with respect to \\(\\theta_j\\) to get\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial^2 w_i(\\theta)}{\\partial \\theta_j^2} t_i (x) \\right ) h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) h(x) \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) h(x) c(\\theta) \\sum_{i=1}^k \\left( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i(x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\frac{\\partial^2 \\ln c(\\theta)}{\\partial \\theta_j^2} \\int_x h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    + & \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    + & \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) c(\\theta) \\sum_{i=1}^k \\left( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i(x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    &= 0\\\\\n\\end{aligned}\n\\end{equation*}\\]\nWe then multiply the second and fifth terms by \\(\\frac{c(\\theta)}{c(\\theta)}\\) and use the knowledge that \\(\\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} = \\frac{\\partial c(\\theta)}{\\partial \\theta_j} \\frac{1}{c(\\theta)}\\) to get\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial^2 w_i(\\theta)}{\\partial \\theta_j^2} t_i (x) \\right ) h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\int_x  \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) h(x) c(\\theta) \\frac{\\partial \\ln c (\\theta)}{\\partial \\theta_j}\\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\int_x  \\left ( \\sum_{i=1}^k \\left ( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i (x) \\right ) \\right )^2 h(x) c(\\theta)  \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right ) dx \\\\\n    + &\\frac{\\partial^2 \\ln c(\\theta)}{\\partial \\theta_j^2} \\int_x h(x) c(\\theta) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    + & \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) c(\\theta) \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    + & \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\int_x h(x) c(\\theta) \\sum_{i=1}^k \\left( \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j} t_i(x) \\right ) \\exp \\left ( \\sum_{i=1}^k w_i(\\theta) t_i(x) \\right )  dx \\\\\n    &= 0\\\\\n\\end{aligned}\n\\end{equation*}\\]\nAfter simplifying and rearranging we get\n\\[\\begin{equation*}\n\\begin{aligned}\n    &E \\left (  \\sum_{i=1}^k \\frac{\\partial^2 w_i(\\theta)}{\\partial \\theta_j^2} t_i (x) \\right )\n    + \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right )\n    + E \\left [ \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right )^2 \\right ]\\\\\n    + &\\frac{\\partial^2 \\ln c(\\theta)}{\\partial \\theta_j^2}\n    + \\left (\\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\right)^2\n    + \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right )\n    = 0\\\\\n\\end{aligned}\n\\end{equation*}\\]\nUsing results from part (a) and simplifying, see that this is equivalent to\n\\[\\begin{equation*}\n    E \\left (  \\sum_{i=1}^k \\frac{\\partial^2 w_i(\\theta)}{\\partial \\theta_j^2} t_i (x) \\right )\n    - \\left ( \\frac{\\partial \\ln c(\\theta)}{\\partial \\theta_j} \\right ) ^2\n    + E \\left [ \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right )^2 \\right ]\n    + \\frac{\\partial^2 \\ln c(\\theta)}{\\partial \\theta_j^2}\n    = 0\\\\\n\\end{equation*}\\]\nWe know that \\(\\textrm{var}(Q) = E(Q^2) - (EQ)^2\\) for a random variable \\(Q\\). Thus we get\n\\[\\begin{equation*}\n    \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) = - \\frac{\\partial^2}{\\partial \\theta_j^2} \\ln c(\\theta) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 w_i (\\theta)}{\\partial \\theta_j^2} t_i(x) \\right )\n\\end{equation*}\\]\n\n\n\nExamples\nIn this section, we will work through two examples to illustrate how to determine if a pdf or pmf of a random variable can be expressed as an exponential family. We then use the above theorem to find the mean and variance.\nExample 1: Let \\(X \\sim \\textrm{Poisson} (\\lambda)\\). We can use the theorem to show that \\(E(X) = \\lambda\\) and \\(\\textrm{var}(X) = \\lambda\\).\nThe pmf of the Poisson distribution is given by\n\\[\\begin{equation*}\n    p(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\end{equation*}\\]\nwhere \\(x = 0,1,...\\) and \\(X\\) represents the number of events that occur in an interval of time.\nWe can show that \\(p(x)\\) can be expressed as an exponential family.\n\\[\\begin{equation*}\n\\begin{aligned}\n    p(x) &= \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n    &= \\frac{1}{x!} e^{-\\lambda} \\lambda^x  \\\\\n    &= \\frac{1}{x!} e^{-\\lambda} e^{x \\ln \\lambda}  \\\\\n\\end{aligned}\n\\end{equation*}\\]\nTherefore, we can conclude this pmf is an exponential family with\n\\(h(x) = \\frac{1}{x!}, c(\\lambda) = e^{-\\lambda}, t_1(x) = x,\\) and \\(w_1(\\lambda) = \\ln \\lambda\\).\nWe can use part (a) of the theorem to find \\(E(X)\\). See that\n\\[\\begin{equation*}\n\\begin{aligned}\n    E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) &= E \\left ( \\sum_{i=1}^k \\frac{\\partial \\ln \\lambda}{\\partial \\lambda} x \\right ) \\\\\n    &= E \\left ( \\sum_{i=1}^k  \\frac{x}{\\lambda} \\right ) \\\\\n    &= - \\frac{\\partial}{\\partial \\theta_j} \\ln c(\\theta) \\\\\n    &= - \\frac{\\partial}{\\partial \\lambda} \\ln (e^{-\\lambda}) \\\\\n    &= - \\frac{\\partial}{\\partial \\lambda} (-\\lambda) \\\\\n    &= 1 \\\\\n\\end{aligned}\n\\end{equation*}\\]\nThus, we get\n\\[\\begin{equation*}\n\\begin{aligned}\n        \\frac{E(X)}{\\lambda} &= 1 \\\\\n        E(X) &= \\lambda\n\\end{aligned}\n\\end{equation*}\\]\nWe can use part (b) of the theorem to find \\(\\textrm{var}(X)\\). We have\n\\[\\begin{equation*}\n    \\begin{aligned}\n        \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) &= \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial \\ln \\lambda}{\\partial \\lambda} x \\right ) \\\\\n        &= \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{x}{\\lambda} \\right ) \\\\\n        &= - \\frac{\\partial^2}{\\partial \\theta_j^2} \\ln c(\\theta) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 w_i (\\theta)}{\\partial \\theta_j^2} t_i(x) \\right ) \\\\\n        &= - \\frac{\\partial^2}{\\partial \\lambda^2} \\ln (e^{-\\lambda}) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 \\ln \\lambda}{\\partial \\lambda^2} x \\right ) \\\\\n        &= - \\frac{\\partial^2}{\\partial \\lambda^2} (-\\lambda) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 \\ln \\lambda}{\\partial \\lambda^2} x \\right ) \\\\\n        &= 0 - E \\left ( \\sum_{i=1}^k \\frac{-x}{\\lambda^2}\\right ) \\\\\n        &= \\frac{1}{\\lambda}\n    \\end{aligned}\n\\end{equation*}\\]\nThus, we get\n\\[\\begin{equation*}\n\\begin{aligned}\n        \\frac{1}{\\lambda^2}\\textrm{var}(X) &= \\frac{1}{\\lambda} \\\\\n        \\textrm{var}(X) &= \\lambda\n\\end{aligned}\n\\end{equation*}\\]\nExample 2: Let \\(X \\sim \\textrm{b} (n,p)\\) with \\(n\\) fixed. We can use the theorem to show that \\(E(X) = np\\) and \\(\\textrm{var}(X) = np(1-p)\\).\nThe pmf of the binomial distribution is given by\n\\[\\begin{equation*}\n    p(x) = \\begin{pmatrix} n \\\\ x\\end{pmatrix} p^x (1-p)^{n-x}\n\\end{equation*}\\]\nwhere \\(x = 0,1,...,n\\) and \\(X\\) represents the number of successes among \\(n\\) trials.\nWe can show that \\(p(x)\\) can be expressed as an exponential family.\n\\[\\begin{equation*}\n\\begin{aligned}\n    p(x) &= \\begin{pmatrix} n \\\\ x\\end{pmatrix} p^x (1-p)^{n-x} \\\\\n    &= \\begin{pmatrix} n \\\\ x\\end{pmatrix} p^x (1-p)^n \\frac{1}{(1-p)^{x}} \\\\\n    &= \\begin{pmatrix} n \\\\ x\\end{pmatrix} (1-p)^n \\left ( \\frac{p}{1-p} \\right ) ^x \\\\\n    &= \\begin{pmatrix} n \\\\ x\\end{pmatrix} (1-p)^n e^{x \\ln \\left ( \\frac{p}{1-p} \\right )} \\\\\n\\end{aligned}\n\\end{equation*}\\]\nTherefore, we can conclude this pmf is an exponential family with\n\\(h(x) = \\begin{pmatrix} n \\\\ x\\end{pmatrix}, c(p) = (1-p)^n, t_1(x) = x,\\) and \\(w_1(p) = \\ln \\left ( \\frac{p}{1-p} \\right )\\).\nWe can use part (a) of the theorem to find \\(E(X)\\). See that\n\\[\\begin{equation*}\n\\begin{aligned}\n    E \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) &= E \\left ( \\sum_{i=1}^k \\frac{\\partial}{\\partial p} \\left ( \\ln \\frac{p}{1-p} \\right ) x \\right ) \\\\\n    &= E \\left ( \\sum_{i=1}^k  \\frac{x}{p-p^2} \\right ) \\\\\n    &= - \\frac{\\partial}{\\partial \\theta_j} \\ln c(\\theta) \\\\\n    &= - \\frac{\\partial}{\\partial p} \\ln (1-p)^n \\\\\n    &= \\frac{n}{1-p} \\\\\n\\end{aligned}\n\\end{equation*}\\]\nThus, we get\n\\[\\begin{equation*}\n\\begin{aligned}\n        \\frac{E(X)}{p - p^2} &= \\frac{n}{1-p} \\\\\n        E(X) &= np\n\\end{aligned}\n\\end{equation*}\\]\nWe can use part (b) of the theorem to find \\(\\textrm{var}(X)\\). We have\n\\[\\begin{equation*}\n    \\begin{aligned}\n        \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial w_i (\\theta)}{\\partial \\theta_j} t_i(x) \\right ) &= \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{\\partial }{\\partial p}  \\left ( \\ln \\frac{p}{1-p} \\right ) x \\right ) \\\\\n        &= \\textrm{var} \\left ( \\sum_{i=1}^k \\frac{x}{p-p^2} \\right ) \\\\\n        &= - \\frac{\\partial^2}{\\partial \\theta_j^2} \\ln c(\\theta) - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2 w_i (\\theta)}{\\partial \\theta_j^2} t_i(x) \\right ) \\\\\n        &= - \\frac{\\partial^2}{\\partial p^2} \\ln (1-p)^n - E \\left ( \\sum_{i=1}^k \\frac{\\partial^2}{\\partial p^2} \\left ( \\ln \\frac{p}{1-p} \\right ) x \\right ) \\\\\n        &= \\frac{n}{(1-p)^2} - E \\left ( \\sum_{i=1}^k \\frac{2p-1 (x)}{(p-p^2)^2}\\right ) \\\\\n        &= \\frac{n}{(1-p)^2} - np \\left ( \\frac{2p-1}{(p-p^2)^2} \\right ) \\\\\n        &= \\frac{np^2 - 2np^2 + np}{(p-p^2)^2} \\\\\n        &= \\frac{np(1-p)}{(p-p^2)^2} \\\\\n    \\end{aligned}\n\\end{equation*}\\]\nThus, we get\n\\[\\begin{equation*}\n\\begin{aligned}\n        \\frac{1}{(p-p^2)^2}\\textrm{var}(X) &= \\frac{np(1-p)}{(p-p^2)^2} \\\\\n        \\textrm{var}(X) &= np(1-p)\n\\end{aligned}\n\\end{equation*}\\]\n\n\nConclusion\nIn summary, we defined an exponential family as a set of probability distributions whose pdfs and pmfs can be expressed in a certain form, making them especially convenient to work with. We introduced a theorem that gave the mean and variance of a random variable belonging to an exponential family in terms of first and second derivatives. We then gave a two-part proof of this theorem.\nWe finally worked through two examples to illustrate the theorem using the Poisson distribution and the binomial distribution with a fixed number of trials. Note that the exponential family also includes other distributions such as the normal and gamma distributions, which can be illustrated in a similar fashion to the examples included here."
  },
  {
    "objectID": "posts/wine_experiment.html",
    "href": "posts/wine_experiment.html",
    "title": "Wine Tasting Analysis Using Full Factorial Design",
    "section": "",
    "text": "This project was done in collaboration with Stephanie Lu that illustrates a full factorial design including data collection and analysis.\n\nIntroduction\nHumans began drinking alcohol over 10,000 years ago. Today, wine is the second most consumed alcoholic beverage around the world, second only to beer, with a global market size of over $450 billion. The different types of wine are vast, creating a disparity in price and quality. In addition to the wine-making process itself, serving conditions are pivotal to wine enjoyment. Preferences are often shaped by cultural norms, such as whites before reds and typical serving temperatures. Furthermore, consumer perception of value plays a role in satisfaction. Insights could inform wine serving strategies, marketing, and product development. This study hopes to investigate traditional serving norms and explore how factors that contribute to overall taste intersect.\nThe objective of this experiment is to investigate how three factors - type of wine (red versus white), price (expensive versus cheap), and temperature (chilled versus room temperature) - impact taster satisfaction. Taster satisfaction is assessed using seven blocks using seven different taste testers (T1–T7), each providing a satisfaction score ranging from 1 (“awful”) to 3 (“amazing”).\nSpecifically, the study’s objectives are to determine if the three individual factors significantly influence taster satisfaction, evaluate potential interactions between these factors (e.g., does the effect of temperature differ for red vs. white wine?), identify which factor or combination of factors most strongly contributes to overall satisfaction ratings, and provide insights into consumer preferences based on sensory evaluations with potential implications for serving and marketing wine.\n\n\nMethods\n\nExperimental Design\nWe employed a full factorial design to investigate the effects of three factors, each with two levels, on taster satisfaction:\n\n\n\n\n\nThis design involves testing all possible combinations of these factors and their levels, resulting in \\(2^3 = 8\\) unique experimental runs.\nThe experiment included the following design features:\n\nBlocking: Seven tasters are treated as seven blocks because each taster’s unique preferences, palate sensitivity, and potential biases introduce a potentially large source of variation of satisfaction ratings. Treating the tasters as blocks deals with this nuisance factor that is not of particular interest in this study. Each of the eight wine samples are applied to each block (i.e., all seven tasters scored all eight wine samples).\nRandomization: The order of the eight experimental runs is randomized to minimize potential order effects and biases. R’s sample function is utilized to randomly select the order of wine samples to be served.\n\n\n\nData Collection Procedure\nSeven tasters participated in total, with each taster providing a satisfaction score for each of the eight samples. All tasters were blinded to the identity and attributes of the wines they evaluated to ensure that their ratings were based solely on sensory experience, thereby minimizing external influence. During each sampling, blindfolded tasters sampled the wine simultaneously in silence and indicated their satisfaction by holding up a number between 1 and 3, where 1 represented “awful”, 2 represented “neutral”, and 3 represented “amazing”. This process was designed to capture immediate satisfaction responses without being influenced by others’ scores.\nThe table below illustrates the design matrix for our wine tasting experiment, including raw wine satisfaction scores from all seven tasters.\n\n\n\n\n\n\n\nStatistical Analysis\nData pre-processing and statistical analysis were conducted in R. Factor levels of “-” and “+” as shown in design matrix were coded as “-1” and “1”, respectively. For analysis, the lm function was called (satisfaction scores as a function of A, B, C (including two- and three-factor interactions), and the block effect) to assess the significance of each term. Factorial effects were subsequently computed by multiplying the regression estimates by 2. Residual analysis was performed to ensure model assumptions were appropriately met.\n\n\n\nResults\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figures above show boxplots of type, price, and temperature, respectively, by taster satisfaction. The boxplot of type compares white wine and red wine, showing the distribution of red wine satisfaction scores are skewed towards higher scores compared to white wines. The boxplot of price compares cheap wines and expensive wines, illustrating cheap wines have a wider spread of scores. The boxplot of serving temperature compares chilled samples and room-temperature samples, illustrating the wider spread of scores for room temperature samples. All three boxplots show that the levels for each of the three factors have the same median satisfaction scores and give some basic information about possible outliers, which will be explored in further analysis.\nThe figure below is a heatmap that illustrates cheap wines achieved higher average satisfaction scores when chilled and expensive wines achieved higher average satisfaction scores when room temperature. Furthermore, chilled expensive white wines received the lowest satisfaction scores, which challenges anecdotal evidence of this being a popular choice among wine drinkers.\n\n\n\n\n\n\n\nSignificant Effects\n\n\n\n\n\nThe above table summarizes the initial model output that includes all possible variables. The only significant term is \\(BC\\), or the two-factor interaction of price and temperature, with a p-value of 0.018. Note that no main effects or other interaction effects are significant. Moreover, the block effect was not significant, signifying there was no significant difference in the mean responses from the testers. This suggests that block-specific factors (e.g., personal preference and taste differences) did not influence the wine satisfaction scores.\n\n\n\n\n\nThe figure above shows the interaction plots of price (\\(B\\)) and temperature (\\(C\\)), with the positions of the factors swapped in each plot to illustrate there are no major discrepancies that would need to be further investigated. For cheap wines, higher mean satisfaction scores were achieved when served chilled. On the other hand, expensive wines were scored higher on average when served at room temperature.\n\n\n\n\n\nThe half-normal plot shown above visualizes absolute factorial effects. As shown by our previous analysis, the interaction effect of \\(BC\\), or price and temperature, is shown to be significant.\n\n\nResidual Analysis\nResidual analysis was conducted to ensure model assumptions were appropriately met. A residuals versus fitted plot is shown below. The major concerns revealed here are the presence of three outliers. However, due to the nature of our experiment and data collection, no data points are removed. The Q-Q residuals plot below shows that the assumption of normality is reasonable.\n\n\n\n\n\n\n\n\n\n\nNote that a log and square root transformation were applied to the response variable, with no major changes in the model or results. Therefore, the response variable was kept as is for purposes of easier model interpretation.\n\n\n\nDiscussion\nWhile the results provide valuable insights, there are several factors and limitations worth considering. In traditional wine-tasting settings, white wines are typically tasted before red wines to prevent the heavier flavors of reds from influencing perceptions of whites. However, in this study, the randomized presentation of wine types led to deviations from standard tasting practices. While this randomization was necessary to reduce order effects, it may have inadvertently affected satisfaction scores, particularly for participants familiar with or sensitive to traditional tasting protocols.\nAdditionally, the typical serving temperatures of wines — whites chilled and reds at room temperature — align with the factor levels tested in this experiment. This alignment may reinforce pre-existing preferences, as tasters might be accustomed to these serving norms.\nThe price factor aimed to examine whether the expense of a wine impacts satisfaction. While the “expensive” wine used in the study was nearly three times the cost of the “cheap” wine, it may not meet the expectations of truly high-end wines. As a result, the findings may be less distinct than if we had utilized the true extremes of the wine market.\nAlthough tasters were treated as blocks to control for individual differences, the relatively small number of participants limits the generalizability of the results. Increasing the number of tasters and trials could enhance the robustness of the findings. Future experiments could also include tasters with varying levels of wine expertise to explore how familiarity impacts satisfaction. Moreover, follow-up experiments can randomize the order of wines within each individual block.\nStatistical analysis revealed that the only variable significantly affecting wine satisfaction scores is the interaction effect of price and temperature. Cheap wines are shown to be best served chilled while expensive wines are best served room temperature. The cooler serving temperatures of wines can help hide flaws, such as an imbalance in tannin, acidity, alcohol, body. This imbalance may be more pronounced in cheaper wines, which often have less strict wine-making requirements. Of course, this idea would require follow-up experiments to test.\nFurthermore, we must keep in mind the Effect Heredity Principle, which states that in order for an interaction to be significant, at least one of its parent factors must be significant. Here, neither the main effects of price nor temperature were found to be significant. By this principle, we cannot confidently arrive at the conclusion that the interaction effect of price and temperature significantly affect wine satisfaction. The significance of this interaction effect can potentially be due to noise in our data. A confirmatory experiment should be done to retest these factors.\nThis current experiment can also serve as a screening experiment, in which the factors of type, price, and temperature can be ruled out and new factors (e.g., testing boxed, bottled, and canned wines) can be tested in subsequent experiments to further deepen our understanding of consumer wine preferences. Alternatively, expanding the range of levels for temperature, price, or wine type (e.g., introducing sparkling or dessert wines) could provide a broader understanding of consumer preferences.\nDespite its limitations, this experiment underscores the importance of sensory and contextual factors in shaping wine satisfaction. By challenging traditional norms and exploring alternative presentations, the findings contribute to a growing body of knowledge that can inform serving practices, marketing strategies, and consumer education in the wine industry.\n\n\nSupplementary Figures\n\n\n\n\n\nMain Effects Plots\n\n\n\n\n\n\n\n\n\nInteraction Plot of Type and Price\n\n\n\n\n\n\n\n\n\nInteraction Plot of Type and Temperature\n\n\n\n\n\n\n\n\n\nResidual Analysis: Residuals vs. Tester/Block"
  },
  {
    "objectID": "posts/bike_sharing.html",
    "href": "posts/bike_sharing.html",
    "title": "Bike-Sharing Analysis Using Kernel Regression",
    "section": "",
    "text": "The objective of this analysis is to illustrate kernel density estimation and a generalized additive model using kernel regression.\n\nIntroduction\nBike-sharing systems provide a relatively new generation of transportation, in which users can conveniently rent and return bikes at various stations throughout many different cities for shared use. In 2022, bike-sharing systems were available in approximately 3000 cities around the world.\nImplications of the rise in popularity of bike-sharing include those related to traffic, the environment, and health. The first bike-sharing systems were initiated to promote non-polluting forms of transportation. Today, people are motivated to use bike-sharing systems for a number of reasons, including to save money or to have a sustainable alternative for short-distance transportation. Bike-sharing systems also have the potential to reduce traffic congestion and positively affect the mental and physical health of users. However, bike-sharing systems have been criticized because they can clutter streets and sidewalks and can divert tax money away from other services.\n\n\nData\nA historical log of bicycle-sharing rental data for 2011 to 2012 was sourced from the UC Irvine Machine Learning Repository. The data comes from Capital Bikeshare, a bicycle-sharing system serving Washington, D.C. that opened in 2010. By February 2011, Capital Bikeshare had 114 stations and by September 2012, they expanded to 288 stations and 2,800 bikes.\n\n\n\n\n\nThe data on the number of bike rentals was aggregated on a daily basis and corresponding weather and seasonal information were gathered. A description of the 16 available variables in the data with 730 observations (one row for each day for two years) is shown in the table above. Explanatory variables of important note include the date, season, year, month, whether or not the day was a holiday, the day of the week, whether or not it was a working day, weather conditions, temperature, feeling temperature, humidity, and wind speed. Here, the response variable is the count of total rented bikes for a given day.\n\n\nEDA\n\n\n\n\n\n\nThere were no missing values in the data. Thus, data cleaning and preparation consisted of converting columns to appropriate data types, e.g. converting the variables season, year, month, holiday, weekday, working day, and weather to factors. Conducting linear regression in R uses these factors as dummy variables, so explicitly encoding the factors for regression was accounted for. Moreover, documentation for the data indicated the wrong codes (1-4) for the season column, e.g. we found that “1” in the season column actually indicated winter and not spring by looking at the month column.\nPreliminary exploratory data analysis is illustrated in the image gallery above. The histogram of the daily rented bike count shows a range of 22 to 8714. Daily rentals that amounted to 4000-5000 occurred the most frequently in the data, with a median daily rental count of 4548.\n\nNumber of Rentals and Time of Year\nFrom the plot of number of daily rented bikes by date, we can deduce that there was an increase in rentals in 2012 and there was a trend for both years of a peak occurring around July.\nTo further investigate these trends, we can look into the number of rentals separated by year, season, and month. It seems that these visualizations can verify some of our initial suppositions while giving us more insights. We can see that 2012 saw an increase in the median number of bike rentals. For both years, a low in rentals occurred in winter, with a high in summer. Looking at individual months for both years, January had the lowest amount of rentals while July had the highest median number of rentals. We can hypothesize that the increase in 2012 may be due to company growth and expansion and the increase during the summer can be associated with season-related weather conditions and temperature.\nFurther, we can see that the number of rentals were fairly even across all days of the week and that the median number of rentals was higher during non-holidays.\n\n\nNumber of Rentals and Weather\nThe above image gallery also visualizes the number of rentals by weather quality, which is denoted clear or cloudy, misty, or light rain or snow. These results make intuitive sense, assuming most users are more inclined to use bikes on clear days and may look at alternative forms of transportation during rain or snow.\nThe original variables temperature and feeling temperature in the data were normalized. Based on the documentation for the data, we calculated the real temperature by multiplying by 41 and feeling temperature by multiplying by 50 for easier interpretation during preliminary data analysis. We followed a similar procedure to calculate the real humidity and wind speed from the original variables. Based on the histograms of temperature and feeling temperature, the temperature ranged from around 2 to 35 degrees Celsius with the feeling temperature ranging from around 4 to 42. Most temperatures were around 10 to 30 degrees. The histogram of the humidity shows a left skew with the most occurrence of around 60%. The histogram of the wind speed shows a right-skew with the median around 12 mph.\nBased on the plot of daily rentals vs. temperature,it appears that there is a general positive relationship between the number of rentals and the temperature. However, as temperatures rise to 30 degrees Celsius or above, there is a negative relationship with the number of rentals. To look into this further, we turn to the next plot of temperature by season, which makes intuitive sense as temperatures peak in the summer. Moreover, it also gives us some initial insight as to why rentals increase during certain months and seasons, which can possibly show the strong correlation among rentals and season, month, and temperature. The plots of daily rentals vs. humidity and vs. windspeed show some potential outliers but there is no readily apparent relationship between rentals and humidity or wind speed.\n\n\n\nStatistical Analysis\nWe will now conduct a formal analysis and further explore the relationships between the daily number of bike rentals and explanatory variables.\n\nKernel Density Estimation\n\n\n\n\n\n\nIn order to estimate the probability density function of the count of bike rentals, we performed kernel density estimation in R and C. In the first kernel density estimate plot above, the black line denotes the kernel density estimate and the red dashed lines denote the 95% confidence bands.\nOne observation gathered from our initial data analysis was that the number of rentals varied with seasons. We saw that bike rentals had the highest median value during summer and the least during winter. We split the data into four parts for each of the four seasons. For each season, we performed kernel density estimation of the count of bike rentals. Unsurprisingly, we see that during the winter, the number of rentals tended to be lower while during the summer, the number of rentals tended to be higher. Further, we see that there are two peaks in density during the summer occurring at just below 5000 daily rentals and just below 7500 daily rentals, in comparison to a peak in density of around 1800 rentals during the winter. The kernel density estimates for each season individually with their associated 95% confidence intervals are also shown above.\nTo determine if there is a statistically significant difference between the means of the number of rentals for each season, we performed ANOVA. The returned p-value was \\(&lt;2 \\times 10^{-16}\\) and thus we reject the hypothesis that all means are equal and conclude that at least one season is different than the others. For post-hoc analysis, we then performed Tukey’s HSD test, which showed that all seasons are different from each other, with the exception of spring and fall (p-value 0.378).\n\n\n\n\n\n\nTo check the assumptions of normality and homoscedasticity, we refer to the figures of the residuals vs. fitted and the Q-Q plots. Based on the residuals, we can assume homogeneity of variance. Moreover, the Q-Q plot shows roughly a straight line so we can assume normality. However, we can see the presence of outliers from these plots, specifically points 442, 668, and 239.\nWe further investigated these outliers, which corresponded to the dates of March 17, 2012, October 29, 2012, and August 27, 2011. Looking at March 17, 2012, we see an usually high count of 7836 rentals during this winter date. We can potentially attribute this to St. Patrick’s Day. President Obama attended a St. Patrick’s Day celebration in Washington, D.C., the area of service in our data. One hypothesis is that since there were documented street closures during the celebration, many users opted to rent bikes to get around the city. Looking at October 29, 2012, we see that Washington, D.C. experienced a city-wide shutdown due to Hurricane Sandy. This is a likely explanation for the unusually low number of only 22 rentals that day. Lastly, the unusually low number of rentals on August 27, 2011 can potentially be attributed to the effects of Hurricane Irene. Although not directly hit, Washington, D.C. experienced particularly high wind speeds and humidity that day.\n\n\nLinear Regression Model\nFor initial modeling to predict the count of rental bikes on a given day, we turn to linear regression. We split the data into training and testing sets and created multiple models with different combinations of explanatory variables. We note that some of the explanatory variables in the data may be highly correlated, such as season and month and temperature and feeling temperature. To measure multicollinearity, we used R to calculate VIF values. We found, for example, season had a VIF value of 166 and month had a value of 340. Thus, we dropped the month variable and our remaining features had VIF values of less than 3.\nWe landed on our final model, which was chosen with \\(R^2\\), RMSE, AIC, and interpretability in mind. The explanatory variables used are season, year, weather quality, and feeling temperature, which are all statistically significant at the 1% level. Moreover, we see that the adjusted \\(R^2\\) value is 0.8118, indicating a good fit. Using the test data, we got a RMSE of 915.4.\nLooking at the residuals and Q-Q plots of the linear model, we see that assumptions are met as there is no pattern in the residuals vs. fitted plot and the Q-Q plot resembles a roughly straight line.\n\n\nGeneralized Additive Model\nAlthough our linear model showed a good fit, we decided to also fit a general additive model to predict the count of daily bike rentals to allow for non-linear relationships. We used the numeric variables feeling temperature, humidity, and wind speed as the predictors. Note that when we fit the generalized additive model using kernel regression, we use only numeric variables due to the limitations of this analysis. The generalized additive model is of the form \\(\\textrm{count} = f_1(\\textrm{temperature}) + f_2(\\textrm{humidity}) + f_3(\\textrm{windspeed}) + \\textrm{noise}\\).\nWe fit the functions \\(f_1, f_2, f_3\\) using kernel regression using residuals from the previous estimate. That is, we fit \\(f_1\\) using kernel regression and fit \\(f_2\\) using the resulting residuals from the previous step. We repeat this process to fit \\(f_3\\).\n\n\n\n\n\n\n\nThe kernel regression estimate of daily rentals is shown above using feeling temperature with the associated bootstrap 95% confidence intervals, followed by a histogram of the residuals. These residuals are used in the second kernel regression using humidity as the predictor, shown in the next kernel regression estimate plot. A histogram follows of the residuals of the second kernel smoothing, which are used in the third kernel regression, using wind speed as the predictor.\nWe also used R’s gam() function in the mgcv package to fit a general additive model using penalized regression splines. Using smoothed feeling temperature, humidity, and wind speed as predictors for the number of daily rental bikes, our generalized additive model returned an adjusted \\(R^2\\) value of 0.608 with all predictors statistically significant at the 1% level. To compare, we ran a linear regression model with the same predictors and got an \\(R^2\\) value of 0.469. Only considering these variables, it appears that the relationship between the response and the explanatory variables is better explained with a non-linear relationship.\nFinally, we created another generalized additive model using the gam() function. We used season, year, weather quality, and smoothed feeling temperature to predict the number of daily rental bikes to compare it to our final linear model. With all terms significant at the 1% level, this generalized additive model returned an adjusted \\(R^2\\) value of 0.866. Due to the higher \\(R^2\\) value, a lower RMSE, and a lower AIC value, we chose the general additive model using season, year, weather quality, and feeling temperature as our final model. A summary of the models is included below.\n\n\n\n\n\n\n\n\nConclusion\nOur analysis showed that the number of daily rentals increased from 2011 to 2012, with year being a significant predictor for the number of rentals. If Capital Bikeshare continues to grow and expands the number of its stations and bikes, it would be important to consider year as a predictor when analyzing data from subsequent years.\nFurther, season was a statistically significant predictor in our final linear model, with summer seeing the highest amounts of bike-sharing rentals. Capital Bikeshare could use this information for decision-making related to operations, marketing, and strategy during certain seasons.\nWeather and temperature were also significant factors to consider. If forecasts show that an upcoming season will expect particularly extreme weather conditions, such as low or high temperatures or precipitation, the company can expect the number of daily rentals to be affected and plan accordingly.\nFuture areas of research include gathering data from more recent years and exploring different models. It would also be interesting to obtain data from other bike-sharing companies to see if our conclusions vary among different companies across different cities. Future analyses could also include hourly rentals instead of daily rentals to see how time of the day affects the number of rentals of shared bikes."
  },
  {
    "objectID": "skills.html#visualization-toolsdatabase-managementother",
    "href": "skills.html#visualization-toolsdatabase-managementother",
    "title": "Skills",
    "section": "Visualization Tools/Database Management/Other",
    "text": "Visualization Tools/Database Management/Other\n\nTableau\nSnowflake\nMySQL\nGit\nExcel\nTeX\nAlteryx\nUiPath"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Please check back for more posts! I am currently in the process of adding more projects that demonstrate both supervised and unsupervised learning."
  },
  {
    "objectID": "portfolio.html#selected-projects",
    "href": "portfolio.html#selected-projects",
    "title": "Portfolio",
    "section": "",
    "text": "Please check back for more posts! I am currently in the process of adding more projects that demonstrate both supervised and unsupervised learning."
  }
]